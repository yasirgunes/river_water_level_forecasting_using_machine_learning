{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and models...\n",
      "\u001b[1m338/338\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step\n",
      "Recreating identical intelligent feature set for XGBoost...\n",
      "\n",
      "--- HYBRID Model Performance ---\n",
      "Day 1 Ahead -> R²: 0.8581 (LSTM Weight: 0.86, XGB Weight: 0.29)\n",
      "Day 2 Ahead -> R²: 0.5522 (LSTM Weight: 0.35, XGB Weight: 0.79)\n",
      "Day 3 Ahead -> R²: 0.3311 (LSTM Weight: -0.19, XGB Weight: 1.29)\n",
      "Day 4 Ahead -> R²: 0.2163 (LSTM Weight: -0.63, XGB Weight: 1.65)\n",
      "Day 5 Ahead -> R²: 0.1652 (LSTM Weight: -0.85, XGB Weight: 1.82)\n",
      "Day 6 Ahead -> R²: 0.1628 (LSTM Weight: -0.98, XGB Weight: 1.92)\n",
      "Day 7 Ahead -> R²: 0.1544 (LSTM Weight: -1.08, XGB Weight: 2.02)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "\n",
    "print(\"Loading data and models...\")\n",
    "# Define split date once\n",
    "split_date_train_end = '2019-01-01'\n",
    "\n",
    "# --- STEP 1: GENERATE PREDICTIONS FROM THE LSTM MODEL ---\n",
    "\n",
    "# 1a. Load data and scaler for LSTM\n",
    "df_lstm = pd.read_csv('combined_dataset.csv', index_col='datetime', parse_dates=True)\n",
    "df_lstm = df_lstm[['stage_m'] + [col for col in df_lstm.columns if col != 'stage_m']]\n",
    "scaler_lstm = joblib.load('multivariate_scaler.joblib') # Assumes this was saved from the best LSTM script\n",
    "data_scaled_lstm = scaler_lstm.transform(df_lstm)\n",
    "\n",
    "# 1b. Create sequences for LSTM\n",
    "N_PAST = 7\n",
    "N_FUTURE = 7\n",
    "def create_lstm_sequences(data, n_past):\n",
    "    X = []\n",
    "    # Go up to the end of the data to get predictions for all possible inputs\n",
    "    for i in range(n_past, len(data) + 1):\n",
    "        X.append(data[i - n_past:i, :])\n",
    "    return np.array(X)\n",
    "X_lstm = create_lstm_sequences(data_scaled_lstm, N_PAST)\n",
    "\n",
    "# 1c. Get LSTM predictions\n",
    "lstm_model = tf.keras.models.load_model('best_multivariate_model.keras')\n",
    "# The predictions will have a length of len(df_lstm) - N_PAST + 1\n",
    "lstm_preds = lstm_model.predict(X_lstm).squeeze()\n",
    "\n",
    "\n",
    "# --- STEP 2: GENERATE PREDICTIONS FROM THE XGBOOST MODEL ---\n",
    "\n",
    "# 2a. Load data and perform IDENTICAL intelligent feature engineering\n",
    "print(\"Recreating identical intelligent feature set for XGBoost...\")\n",
    "df_xgb_base = pd.read_csv('combined_dataset.csv', index_col='datetime', parse_dates=True)\n",
    "df_features = pd.DataFrame(index=df_xgb_base.index)\n",
    "\n",
    "df_features['stage_m_lag_1'] = df_xgb_base['stage_m'].shift(1)\n",
    "df_features['stage_m_lag_2'] = df_xgb_base['stage_m'].shift(2)\n",
    "df_features['stage_m_lag_3'] = df_xgb_base['stage_m'].shift(3)\n",
    "df_features['stage_m_lag_4'] = df_xgb_base['stage_m'].shift(4)\n",
    "df_features['prcp_lag_1'] = df_xgb_base['prcp'].shift(1)\n",
    "df_features['prcp_lag_2'] = df_xgb_base['prcp'].shift(2)\n",
    "df_features['rhum_lag_1'] = df_xgb_base['rhum'].shift(1)\n",
    "df_features['rhum_lag_2'] = df_xgb_base['rhum'].shift(2)\n",
    "df_features['tavg_lag_1'] = df_xgb_base['tavg'].shift(1)\n",
    "df_features['tavg_lag_3'] = df_xgb_base['tavg'].shift(3)\n",
    "df_features['tavg_lag_5'] = df_xgb_base['tavg'].shift(5)\n",
    "df_features['wspd_lag_1'] = df_xgb_base['wspd'].shift(1)\n",
    "df_features['pres_lag_1'] = df_xgb_base['pres'].shift(1)\n",
    "df_features['prcp_rolling_3day_sum'] = df_xgb_base['prcp'].rolling(window=3).sum().shift(1)\n",
    "\n",
    "# 2b. Create XGBoost X and y, and drop NaNs\n",
    "X_xgb = df_features.copy()\n",
    "y_xgb_targets = pd.DataFrame()\n",
    "for i in range(1, N_FUTURE + 1):\n",
    "    y_xgb_targets[f'target_day_{i}'] = df_xgb_base['stage_m'].shift(-i)\n",
    "\n",
    "# Combine and drop NaNs created by shifting\n",
    "df_full_xgb = pd.concat([X_xgb, y_xgb_targets], axis=1)\n",
    "df_full_xgb.dropna(inplace=True)\n",
    "X_xgb = df_full_xgb[X_xgb.columns]\n",
    "y_xgb = df_full_xgb[y_xgb_targets.columns]\n",
    "\n",
    "# 2c. Split and Scale the XGBoost data correctly (No Data Leakage)\n",
    "train_indices = X_xgb.index <= split_date_train_end\n",
    "X_xgb_train, X_xgb_full = X_xgb[train_indices], X_xgb # Keep full X_xgb for prediction\n",
    "y_xgb_train = y_xgb[train_indices]\n",
    "\n",
    "scaler_xgb = StandardScaler()\n",
    "X_xgb_train_scaled = scaler_xgb.fit_transform(X_xgb_train)\n",
    "X_xgb_full_scaled = scaler_xgb.transform(X_xgb_full) # Transform the whole set\n",
    "\n",
    "# 2d. Train XGBoost and get predictions for the ENTIRE dataset\n",
    "xgb_model = MultiOutputRegressor(estimator=xgb.XGBRegressor(objective='reg:squarederror', n_estimators=109, learning_rate=0.05, max_depth=6, min_child_weight=4, subsample=1.0, colsample_bytree=0.9, n_jobs=-1, random_state=42))\n",
    "xgb_model.fit(X_xgb_train_scaled, y_xgb_train)\n",
    "xgb_preds = xgb_model.predict(X_xgb_full_scaled)\n",
    "\n",
    "\n",
    "# --- STEP 3: ASSEMBLE THE META-DATASET AND TRAIN THE HYBRID MODEL ---\n",
    "\n",
    "# 3a. Align all data\n",
    "# The ground truth `y`\n",
    "y_true_df = pd.DataFrame()\n",
    "for i in range(1, N_FUTURE + 1):\n",
    "    y_true_df[f'target_day_{i}'] = df_lstm['stage_m'].shift(-i)\n",
    "\n",
    "# Inverse scale the LSTM predictions first\n",
    "n_features_lstm = data_scaled_lstm.shape[1]\n",
    "dummy_preds_lstm = np.zeros((len(lstm_preds.flatten()), n_features_lstm))\n",
    "dummy_preds_lstm[:, 0] = lstm_preds.flatten()\n",
    "lstm_preds_unscaled = scaler_lstm.inverse_transform(dummy_preds_lstm)[:, 0].reshape(lstm_preds.shape)\n",
    "\n",
    "# Create dataframes from our predictions\n",
    "# THIS IS THE FIX: The index should match the length of the predictions.\n",
    "# It starts at the first possible prediction date (index N_PAST - 1) and goes to the end.\n",
    "lstm_index_start = N_PAST - 1\n",
    "correct_lstm_index = df_lstm.index[lstm_index_start : lstm_index_start + len(lstm_preds_unscaled)]\n",
    "\n",
    "lstm_preds_df = pd.DataFrame(\n",
    "    lstm_preds_unscaled,\n",
    "    index=correct_lstm_index,\n",
    "    columns=[f'lstm_pred_day_{i+1}' for i in range(N_FUTURE)]\n",
    ")\n",
    "# The xgb_preds_df line is correct as is\n",
    "xgb_preds_df = pd.DataFrame(xgb_preds, index=X_xgb.index, columns=[f'xgb_pred_day_{i+1}' for i in range(N_FUTURE)])\n",
    "\n",
    "# 3b. Join everything together\n",
    "meta_df = y_true_df.join(lstm_preds_df).join(xgb_preds_df)\n",
    "meta_df.dropna(inplace=True)\n",
    "\n",
    "# 3c. Train and Evaluate the Hybrid Model day by day\n",
    "print(\"\\n--- HYBRID Model Performance ---\")\n",
    "for i in range(N_FUTURE):\n",
    "    day = i + 1\n",
    "    \n",
    "    # Create the dataset for this specific day\n",
    "    X_meta = meta_df[[f'lstm_pred_day_{day}', f'xgb_pred_day_{day}']]\n",
    "    y_meta = meta_df[f'target_day_{day}']\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_indices_meta = X_meta.index <= split_date_train_end\n",
    "    val_indices_meta = ~train_indices_meta\n",
    "    \n",
    "    X_meta_train, X_meta_val = X_meta[train_indices_meta], X_meta[val_indices_meta]\n",
    "    y_meta_train, y_meta_val = y_meta[train_indices_meta], y_meta[val_indices_meta]\n",
    "    \n",
    "    # Train the meta-model\n",
    "    meta_model = LinearRegression()\n",
    "    meta_model.fit(X_meta_train, y_meta_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    hybrid_predictions = meta_model.predict(X_meta_val)\n",
    "    r2 = r2_score(y_meta_val, hybrid_predictions)\n",
    "    \n",
    "    # Print results and the learned weights!\n",
    "    lstm_weight = meta_model.coef_[0]\n",
    "    xgb_weight = meta_model.coef_[1]\n",
    "    print(f\"Day {day} Ahead -> R²: {r2:.4f} (LSTM Weight: {lstm_weight:.2f}, XGB Weight: {xgb_weight:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
