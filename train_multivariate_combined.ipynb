{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 18:28:16.349543: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-19 18:28:16.431376: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750346896.480050  115610 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750346896.492039  115610 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750346896.595740  115610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750346896.595792  115610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750346896.595794  115610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750346896.595796  115610 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-19 18:28:16.613696: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in order: ['stage_m', 'discharge_cms', 'tavg', 'prcp', 'wspd', 'pres', 'rhum']\n",
      "Training period: 1995-10-01 00:00:00 to 2018-04-20 00:00:00\n",
      "Validation period: 2019-01-01 00:00:00 to 2025-06-01 00:00:00\n",
      "Training samples: 8216 (77.8%)\n",
      "Validation samples: 2344 (22.2%)\n",
      "Gap between training and validation: 2019-01-01 - 2018-04-21\n",
      "Scaler saved to 'multivariate_combined_scaler.joblib'\n",
      "\n",
      "New sequence shapes:\n",
      "X_train shape: (8203, 7, 7)\n",
      "y_train shape: (8203, 7)\n",
      "Number of features being used: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750346901.380228  115610 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3539 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">69,632</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ repeat_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │                   │            │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],     │\n",
       "│                     │                   │            │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │     \u001b[38;5;34m69,632\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),     │    \u001b[38;5;34m131,584\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ repeat_vector       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mRepeatVector\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │    \u001b[38;5;34m131,584\u001b[0m │ repeat_vector[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │                   │            │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],     │\n",
       "│                     │                   │            │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │    \u001b[38;5;34m131,584\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │        \u001b[38;5;34m129\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">464,513</span> (1.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m464,513\u001b[0m (1.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">464,513</span> (1.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m464,513\u001b[0m (1.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750346910.214337  115723 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 50ms/step - loss: 0.6786 - val_loss: 0.6319\n",
      "Epoch 2/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.5142 - val_loss: 0.5777\n",
      "Epoch 3/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.4792 - val_loss: 0.5731\n",
      "Epoch 4/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.4631 - val_loss: 0.5580\n",
      "Epoch 5/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - loss: 0.4784 - val_loss: 0.5505\n",
      "Epoch 6/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 0.4823 - val_loss: 0.5525\n",
      "Epoch 7/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.4420 - val_loss: 0.5487\n",
      "Epoch 8/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 0.4464 - val_loss: 0.5609\n",
      "Epoch 9/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.4385 - val_loss: 0.5525\n",
      "Epoch 10/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - loss: 0.4244 - val_loss: 0.5447\n",
      "Epoch 11/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - loss: 0.4482 - val_loss: 0.5409\n",
      "Epoch 12/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.4246 - val_loss: 0.5509\n",
      "Epoch 13/150\n",
      "\u001b[1m65/65\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.4398 - val_loss: 0.5590\n",
      "Epoch 14/150\n",
      "\u001b[1m30/65\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.4125"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import joblib\n",
    "\n",
    "# 1. Load and Prepare MULTIVARIATE Data ## MULTIVARIATE CHANGE ##\n",
    "# Load the combined dataset with weather features\n",
    "df_multi = pd.read_csv('combined_dataset.csv', index_col='datetime', parse_dates=True)\n",
    "\n",
    "# Ensure 'stage_m' is the first column, as our sequence function will use it as the target\n",
    "df_multi = df_multi[['stage_m'] + [col for col in df_multi.columns if col != 'stage_m']]\n",
    "\n",
    "print(\"Columns in order:\", df_multi.columns.tolist())\n",
    "\n",
    "# 2. Split Data with a Gap (following paper methodology, as in context7)\n",
    "# Dataset: 1995-10-01 to 2025-06-01 (~29.67 years)\n",
    "# Paper approach: 76% training, 23% validation with gap\n",
    "# Training: 76% of 29.67 years ≈ 22.55 years → 1995-10-01 to 2018-04-21\n",
    "# Gap: ~8 months (2018-04-21 to 2019-01-01)\n",
    "# Validation: 2019-01-01 to 2025-06-01\n",
    "split_date_train_end = '2018-04-21'\n",
    "split_date_val_start = '2019-01-01'\n",
    "train_data = df_multi.loc[df_multi.index < split_date_train_end]\n",
    "val_data = df_multi.loc[df_multi.index >= split_date_val_start]\n",
    "\n",
    "print(f\"Training period: {train_data.index.min()} to {train_data.index.max()}\")\n",
    "print(f\"Validation period: {val_data.index.min()} to {val_data.index.max()}\")\n",
    "print(f\"Training samples: {len(train_data)} ({len(train_data)/(len(train_data)+len(val_data))*100:.1f}%)\")\n",
    "print(f\"Validation samples: {len(val_data)} ({len(val_data)/(len(train_data)+len(val_data))*100:.1f}%)\")\n",
    "print(f\"Gap between training and validation: {split_date_val_start} - {split_date_train_end}\")\n",
    "\n",
    "# 3. Scale Data ## MULTIVARIATE CHANGE ##\n",
    "# The scaler will now be fit on all features\n",
    "scaler = StandardScaler()\n",
    "scaled_train_data = scaler.fit_transform(train_data)\n",
    "scaled_val_data = scaler.transform(val_data)\n",
    "\n",
    "# Save the fitted scaler using joblib\n",
    "joblib.dump(scaler, 'multivariate_combined_scaler.joblib')\n",
    "print(\"Scaler saved to 'multivariate_combined_scaler.joblib'\")\n",
    "\n",
    "# 4. Create Sequences ## MULTIVARIATE CHANGE ##\n",
    "# The function now takes all columns for X, but still only the first column for y\n",
    "N_PAST = 7    # Use 7 days of history\n",
    "N_FUTURE = 7  # Predict 7 days into the future\n",
    "\n",
    "def create_sequences(data, n_past, n_future):\n",
    "    X, y = [], []\n",
    "    target_col_index = 0 # 'stage_m' is at index 0\n",
    "    for i in range(n_past, len(data) - n_future + 1):\n",
    "        # Input X: takes n_past days of ALL features\n",
    "        X.append(data[i - n_past:i, :])\n",
    "        # Output y: takes n_future days of ONLY the target feature\n",
    "        y.append(data[i:i + n_future, target_col_index])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_sequences(scaled_train_data, N_PAST, N_FUTURE)\n",
    "X_val, y_val = create_sequences(scaled_val_data, N_PAST, N_FUTURE)\n",
    "# Note: No .reshape() is needed for X, as it's already 3D\n",
    "\n",
    "print(f\"\\nNew sequence shapes:\")\n",
    "print(f\"X_train shape: {X_train.shape}\") # Should be (samples, 7, num_features)\n",
    "print(f\"y_train shape: {y_train.shape}\")   # Should be (samples, 7)\n",
    "\n",
    "# 5. Build Stacked LSTM Encoder-Decoder (Same architecture, just n_features will be > 1)\n",
    "n_features = X_train.shape[2]\n",
    "print(f\"Number of features being used: {n_features}\")\n",
    "UNITS = 128\n",
    "DROPOUT_RATE = 0.15\n",
    "\n",
    "inputs = Input(shape=(N_PAST, n_features))\n",
    "encoder_l1 = LSTM(UNITS, return_sequences=True, dropout=DROPOUT_RATE)(inputs)\n",
    "encoder_l2, state_h, state_c = LSTM(UNITS, return_sequences=False, dropout=DROPOUT_RATE, return_state=True)(encoder_l1)\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder_inputs = RepeatVector(N_FUTURE)(encoder_l2)\n",
    "decoder_l1 = LSTM(UNITS, return_sequences=True, dropout=DROPOUT_RATE)(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_l2 = LSTM(UNITS, return_sequences=True, dropout=DROPOUT_RATE)(decoder_l1)\n",
    "output = TimeDistributed(Dense(1))(decoder_l2)\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# 6. Compile Model (Same as our best univariate model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# 7. Train the Model ## MULTIVARIATE CHANGE ##\n",
    "# Using a new filename for the saved model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(filepath='best_multivariate_combined_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=150,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 8. Evaluation and Visualization ## MULTIVARIATE CHANGE ##\n",
    "# The logic for inverse scaling the predictions is now more complex\n",
    "\n",
    "# Load the scaler back (this demonstrates how you'd use it in production)\n",
    "loaded_scaler = joblib.load('multivariate_combined_scaler.joblib')\n",
    "\n",
    "best_model = tf.keras.models.load_model('best_multivariate_combined_model.keras')\n",
    "predictions_scaled = best_model.predict(X_val).squeeze()\n",
    "\n",
    "# --- Special handling for inverse scaling ---\n",
    "# Create a dummy array shaped like the original data the scaler was fit on\n",
    "# We will overwrite the first column (our target) with our predictions\n",
    "dummy_preds = np.zeros((len(predictions_scaled.flatten()), n_features))\n",
    "dummy_preds[:, 0] = predictions_scaled.flatten()\n",
    "# Inverse transform the entire dummy array\n",
    "rescaled_preds = loaded_scaler.inverse_transform(dummy_preds)\n",
    "# Extract just the first column (our rescaled predictions) and reshape\n",
    "predictions_original = rescaled_preds[:, 0].reshape(y_val.shape)\n",
    "\n",
    "# Do the same for the true values\n",
    "dummy_true = np.zeros((len(y_val.flatten()), n_features))\n",
    "dummy_true[:, 0] = y_val.flatten()\n",
    "rescaled_true = loaded_scaler.inverse_transform(dummy_true)\n",
    "y_true_original = rescaled_true[:, 0].reshape(y_val.shape)\n",
    "# --- End of special handling ---\n",
    "\n",
    "print(\"\\n--- MULTIVARIATE Model Performance on Validation Set ---\")\n",
    "for i in range(N_FUTURE):\n",
    "    day = i + 1\n",
    "    mae = mean_absolute_error(y_true_original[:, i], predictions_original[:, i])\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_original[:, i], predictions_original[:, i]))\n",
    "    r2 = r2_score(y_true_original[:, i], predictions_original[:, i])\n",
    "    print(f\"Day {day} Ahead -> MAE: {mae:.4f} m, RMSE: {rmse:.4f} m, R²: {r2:.4f}\")\n",
    "\n",
    "# Plotting code remains the same\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss (MSE)')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss (MSE)')\n",
    "plt.title('Multivariate Model Learning Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "val_dates = val_data.index[N_PAST:len(y_true_original) + N_PAST]\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(val_dates, y_true_original[:, 0], label='Actual Values', color='blue', alpha=0.8)\n",
    "plt.plot(val_dates, predictions_original[:, 0], label='Day 1 Forecast', color='red', linestyle='--', alpha=0.9)\n",
    "plt.plot(val_dates, predictions_original[:, 2], label='Day 3 Forecast', color='orange', linestyle=':', alpha=0.8)\n",
    "plt.plot(val_dates, predictions_original[:, 6], label='Day 7 Forecast', color='green', linestyle='-.', alpha=0.8)\n",
    "plt.title('Multivariate 7-Day Water Level Forecast vs. Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Water Level (m)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
