{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in order: ['stage_m', 'discharge_cms', 'tavg', 'prcp', 'wspd', 'pres', 'rhum']\n",
      "\n",
      "Original sequence shape: (8203, 7, 7)\n",
      "Flattened shape for MLP/XGBoost: (8203, 49)\n",
      "\n",
      "--- Training Simple MLP Model ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">455</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m6,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m455\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,111</span> (59.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,111\u001b[0m (59.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,111</span> (59.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,111\u001b[0m (59.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP training completed in 16.14 seconds.\n",
      "MLP model saved as 'best_baseline_mlp_model.keras'\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\n",
      "--- Training XGBoost Model ---\n",
      "XGBoost training completed in 4.47 seconds.\n",
      "XGBoost model saved as 'best_baseline_xgboost_model.joblib'\n",
      "\n",
      "--- Simple MLP Model Performance on Validation Set ---\n",
      "Day 1 Ahead -> MAE: 0.1504 m, RMSE: 0.2873 m, R²: 0.8570\n",
      "Day 2 Ahead -> MAE: 0.2422 m, RMSE: 0.4543 m, R²: 0.6417\n",
      "Day 3 Ahead -> MAE: 0.3049 m, RMSE: 0.5549 m, R²: 0.4650\n",
      "Day 4 Ahead -> MAE: 0.3377 m, RMSE: 0.5991 m, R²: 0.3756\n",
      "Day 5 Ahead -> MAE: 0.3571 m, RMSE: 0.6250 m, R²: 0.3201\n",
      "Day 6 Ahead -> MAE: 0.3814 m, RMSE: 0.6465 m, R²: 0.2722\n",
      "Day 7 Ahead -> MAE: 0.3884 m, RMSE: 0.6550 m, R²: 0.2519\n",
      "\n",
      "--- XGBoost Model Performance on Validation Set ---\n",
      "Day 1 Ahead -> MAE: 0.1152 m, RMSE: 0.2362 m, R²: 0.9033\n",
      "Day 2 Ahead -> MAE: 0.2333 m, RMSE: 0.4482 m, R²: 0.6514\n",
      "Day 3 Ahead -> MAE: 0.3034 m, RMSE: 0.5538 m, R²: 0.4671\n",
      "Day 4 Ahead -> MAE: 0.3377 m, RMSE: 0.5980 m, R²: 0.3780\n",
      "Day 5 Ahead -> MAE: 0.3573 m, RMSE: 0.6201 m, R²: 0.3307\n",
      "Day 6 Ahead -> MAE: 0.3716 m, RMSE: 0.6367 m, R²: 0.2940\n",
      "Day 7 Ahead -> MAE: 0.3780 m, RMSE: 0.6427 m, R²: 0.2797\n",
      "\n",
      "--- For Comparison: Your Tuned LSTM Results ---\n",
      "Day 1 Ahead -> MAE: 0.1326 m, RMSE: 0.2424 m, R²: 0.8982\n",
      "Day 2 Ahead -> MAE: 0.2431 m, RMSE: 0.4457 m, R²: 0.6553\n",
      "Day 3 Ahead -> MAE: 0.3086 m, RMSE: 0.5467 m, R²: 0.4807\n",
      "Day 4 Ahead -> MAE: 0.3417 m, RMSE: 0.5930 m, R²: 0.3884\n",
      "Day 5 Ahead -> MAE: 0.3606 m, RMSE: 0.6165 m, R²: 0.3385\n",
      "Day 6 Ahead -> MAE: 0.3742 m, RMSE: 0.6312 m, R²: 0.3060\n",
      "Day 7 Ahead -> MAE: 0.3839 m, RMSE: 0.6426 m, R²: 0.2798\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Load and Prepare Data (Same as your LSTM script)\n",
    "# -------------------------------------------------------------------\n",
    "df_multi = pd.read_csv('combined_dataset.csv', index_col='datetime', parse_dates=True)\n",
    "df_multi = df_multi[['stage_m'] + [col for col in df_multi.columns if col != 'stage_m']]\n",
    "\n",
    "print(\"Columns in order:\", df_multi.columns.tolist())\n",
    "\n",
    "split_date_train_end = '2018-04-21'\n",
    "split_date_val_start = '2019-01-01'\n",
    "train_data = df_multi.loc[df_multi.index < split_date_train_end]\n",
    "val_data = df_multi.loc[df_multi.index >= split_date_val_start]\n",
    "\n",
    "# Load the scaler that was fit on this data during LSTM training\n",
    "# This ensures we use the exact same scaling for a fair comparison\n",
    "scaler = joblib.load('multivariate_combined_scaler.joblib')\n",
    "scaled_train_data = scaler.transform(train_data)\n",
    "scaled_val_data = scaler.transform(val_data)\n",
    "\n",
    "N_PAST = 7\n",
    "N_FUTURE = 7\n",
    "n_features = scaled_train_data.shape[1]\n",
    "\n",
    "def create_sequences(data, n_past, n_future):\n",
    "    X, y = [], []\n",
    "    target_col_index = 0\n",
    "    for i in range(n_past, len(data) - n_future + 1):\n",
    "        X.append(data[i - n_past:i, :])\n",
    "        y.append(data[i:i + n_future, target_col_index])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences just like for the LSTM\n",
    "X_train_seq, y_train = create_sequences(scaled_train_data, N_PAST, N_FUTURE)\n",
    "X_val_seq, y_val = create_sequences(scaled_val_data, N_PAST, N_FUTURE)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Reshape Data for MLP and XGBoost\n",
    "# -------------------------------------------------------------------\n",
    "# We flatten the sequence of past data into a single feature vector.\n",
    "# Shape changes from (samples, 7, 6) to (samples, 42)\n",
    "X_train_flat = X_train_seq.reshape(X_train_seq.shape[0], -1)\n",
    "X_val_flat = X_val_seq.reshape(X_val_seq.shape[0], -1)\n",
    "\n",
    "print(f\"\\nOriginal sequence shape: {X_train_seq.shape}\")\n",
    "print(f\"Flattened shape for MLP/XGBoost: {X_train_flat.shape}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Train and Evaluate the Simple MLP Model\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n--- Training Simple MLP Model ---\")\n",
    "\n",
    "# Define MLP architecture\n",
    "inputs = Input(shape=(X_train_flat.shape[1],)) # Input shape is the number of flattened features\n",
    "dense1 = Dense(128, activation='relu')(inputs)\n",
    "dropout1 = Dropout(0.2)(dense1)\n",
    "dense2 = Dense(64, activation='relu')(dropout1)\n",
    "# The output layer has N_FUTURE neurons, one for each day to predict\n",
    "outputs = Dense(N_FUTURE)(dense2) \n",
    "\n",
    "mlp_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "mlp_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "mlp_model.summary()\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "mlp_history = mlp_model.fit(\n",
    "    X_train_flat, y_train,\n",
    "    epochs=150,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val_flat, y_val),\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)],\n",
    "    verbose=0 # Set to 0 to keep output clean\n",
    ")\n",
    "mlp_training_time = time.time() - start_time\n",
    "print(f\"MLP training completed in {mlp_training_time:.2f} seconds.\")\n",
    "\n",
    "# Save the MLP model\n",
    "mlp_model.save('best_baseline_mlp_model.keras')\n",
    "print(\"MLP model saved as 'best_baseline_mlp_model.keras'\")\n",
    "\n",
    "# Predict and evaluate\n",
    "mlp_predictions_scaled = mlp_model.predict(X_val_flat)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Train and Evaluate the XGBoost Model\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n--- Training XGBoost Model ---\")\n",
    "\n",
    "# Instantiate the XGBoost regressor for multi-output regression\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=1000,          # Will stop early\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    early_stopping_rounds=15,   # Stop if validation loss doesn't improve for 15 rounds\n",
    "    n_jobs=-1                   # Use all available CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "xgb_model.fit(\n",
    "    X_train_flat, y_train,\n",
    "    eval_set=[(X_val_flat, y_val)],\n",
    "    verbose=False               # Set to False to keep output clean\n",
    ")\n",
    "xgb_training_time = time.time() - start_time\n",
    "print(f\"XGBoost training completed in {xgb_training_time:.2f} seconds.\")\n",
    "\n",
    "# Save the XGBoost model\n",
    "joblib.dump(xgb_model, 'best_baseline_xgboost_model.joblib')\n",
    "print(\"XGBoost model saved as 'best_baseline_xgboost_model.joblib'\")\n",
    "\n",
    "# Predict and evaluate\n",
    "xgb_predictions_scaled = xgb_model.predict(X_val_flat)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5. Inverse Scale and Compare All Models\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def evaluate_model(model_name, predictions_scaled, y_true_scaled):\n",
    "    \"\"\"A helper function to inverse scale and calculate metrics.\"\"\"\n",
    "    # Create a dummy array for inverse scaling\n",
    "    dummy_preds = np.zeros((len(predictions_scaled.flatten()), n_features))\n",
    "    dummy_preds[:, 0] = predictions_scaled.flatten()\n",
    "    predictions_original = scaler.inverse_transform(dummy_preds)[:, 0].reshape(y_true_scaled.shape)\n",
    "    \n",
    "    dummy_true = np.zeros((len(y_true_scaled.flatten()), n_features))\n",
    "    dummy_true[:, 0] = y_true_scaled.flatten()\n",
    "    y_true_original = scaler.inverse_transform(dummy_true)[:, 0].reshape(y_true_scaled.shape)\n",
    "    \n",
    "    print(f\"\\n--- {model_name} Model Performance on Validation Set ---\")\n",
    "    for i in range(N_FUTURE):\n",
    "        day = i + 1\n",
    "        mae = mean_absolute_error(y_true_original[:, i], predictions_original[:, i])\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_original[:, i], predictions_original[:, i]))\n",
    "        r2 = r2_score(y_true_original[:, i], predictions_original[:, i])\n",
    "        print(f\"Day {day} Ahead -> MAE: {mae:.4f} m, RMSE: {rmse:.4f} m, R²: {r2:.4f}\")\n",
    "\n",
    "# Evaluate our new models\n",
    "evaluate_model(\"Simple MLP\", mlp_predictions_scaled, y_val)\n",
    "evaluate_model(\"XGBoost\", xgb_predictions_scaled, y_val)\n",
    "\n",
    "print(\"\\n--- For Comparison: Your Tuned LSTM Results ---\")\n",
    "print(\"Day 1 Ahead -> MAE: 0.1326 m, RMSE: 0.2424 m, R²: 0.8982\")\n",
    "print(\"Day 2 Ahead -> MAE: 0.2431 m, RMSE: 0.4457 m, R²: 0.6553\")\n",
    "print(\"Day 3 Ahead -> MAE: 0.3086 m, RMSE: 0.5467 m, R²: 0.4807\")\n",
    "print(\"Day 4 Ahead -> MAE: 0.3417 m, RMSE: 0.5930 m, R²: 0.3884\")\n",
    "print(\"Day 5 Ahead -> MAE: 0.3606 m, RMSE: 0.6165 m, R²: 0.3385\")\n",
    "print(\"Day 6 Ahead -> MAE: 0.3742 m, RMSE: 0.6312 m, R²: 0.3060\")\n",
    "print(\"Day 7 Ahead -> MAE: 0.3839 m, RMSE: 0.6426 m, R²: 0.2798\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
